{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 978,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from torchimize.functions import lsq_lma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 979,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 32)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cpu')\n",
    "# For data preprocessing. To be replcaed with AR section.\n",
    "cancer_data = pd.read_csv('/Users/waldo/Documents/23 HT/Data Mining 1DL370/Assignment4/cancer.csv',sep=';')\n",
    "cancer_data.iloc[cancer_data['diagnosis'] == 'M',1] = 1\n",
    "cancer_data.iloc[cancer_data['diagnosis'] == 'B',1] = 0\n",
    "cancer_benign = cancer_data[cancer_data['diagnosis'] == 0]\n",
    "\n",
    "X, y = cancer_data.iloc[:,2:], cancer_data.iloc[:,1]\n",
    "X_normed = pd.DataFrame(MinMaxScaler().fit_transform(X))\n",
    "X = X_normed.astype('float')\n",
    "y = y.astype('float')\n",
    "X = X.values\n",
    "print(X.shape)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.398, random_state=0)\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test.to_numpy(), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 980,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([342, 32])\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 981,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.input_layer = nn.Linear(32, 11)  # 32 input neurons to 11 hidden neurons. This will be changed to actual number of features.\n",
    "        self.hidden_layer = nn.Linear(11, 1)  # 11 hidden neurons to 1 output neuron\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.tanh(self.input_layer(x))  # Apply tanh activation to the input layer\n",
    "        x = torch.sigmoid(self.hidden_layer(x))  # Apply sigmoid activation to the output layer for binary classification\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 982,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = MLP()\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "#for name, param in model.named_parameters():\n",
    "    #print(name, param)\n",
    "\n",
    "# Define the optimizer (Adam optimizer). To be changed to Lev-Mar\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 983,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 44\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, y_train.view(-1, 1))\n",
    "    #print(loss)\n",
    "    # Backward pass and optimization\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()  # Update weights\n",
    "    \n",
    "    #if (epoch + 1) % 1000 == 0:\n",
    "       # print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "print(\"Training completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 984,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test data: 0.9824\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    predictions = model(X_test)\n",
    "    predictions = (predictions > 0.5).float()\n",
    "    accuracy = torch.sum(predictions.view(-1) == y_test).item() / y_test.size(0)\n",
    "    print(f'Accuracy on test data: {accuracy:.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 985,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nloss_list = []\\n\\nclass LevenbergMarquardtOptimizer:\\n    def __init__(self, model, target_func, initial_params, lambda_=0.01, max_iterations=50):\\n        self.model = model\\n        self.target_func = target_func\\n        self.params = torch.nn.Parameter(torch.tensor(initial_params, requires_grad=True))\\n        self.lambda_ = lambda_\\n        self.max_iterations = max_iterations\\n\\n    def optimize(self):\\n        optimizer = torch.optim.SGD([self.params], lr=0.01)  # Dummy optimizer, as we manually perform updates\\n        \\n        for iteration in range(self.max_iterations):\\n            optimizer.zero_grad()\\n            loss = self.target_func(self.model(self.params))\\n            loss_list.append(loss)\\n            print(loss)\\n            loss.backward()\\n\\n            # Compute the approximate Hessian matrix\\n            with torch.no_grad():\\n                approx_hessian = torch.autograd.functional.hessian(self.target_func, self.params)\\n                diag_values = approx_hessian.diagonal() + self.lambda_\\n            # Update parameters using LM formula: theta_new = theta - (H + lambda*I)^(-1) * gradient\\n            update = torch.linalg.solve(approx_hessian + torch.diag(diag_values), -self.params.grad)\\n            self.params.data += update\\n\\n        return self.params.data.numpy()\\n\\n# Example usage:\\n\\n# Define your target function and model appropriately\\ndef target_func(output):\\n    # Define your loss function here, for example, mean squared error\\n    loss = torch.mean((output - y_train.view(-1, 1))**2)\\n    return loss\\n\\n# Instantiate your model and Levenberg-Marquardt optimizer\\nmodel = MLP()\\ninitial_params = torch.empty(X_train.shape[1])\\ninitial_params = nn.init.uniform_(initial_params)\\n#initial_params = torch.tensor([1.0] * 375)  # Initial guess for parameters\\nlm_optimizer = LevenbergMarquardtOptimizer(model, target_func, model.parameters())\\n\\n# Perform optimization\\noptimized_params = lm_optimizer.optimize()\\n'"
      ]
     },
     "execution_count": 985,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "loss_list = []\n",
    "\n",
    "class LevenbergMarquardtOptimizer:\n",
    "    def __init__(self, model, target_func, initial_params, lambda_=0.01, max_iterations=50):\n",
    "        self.model = model\n",
    "        self.target_func = target_func\n",
    "        self.params = torch.nn.Parameter(torch.tensor(initial_params, requires_grad=True))\n",
    "        self.lambda_ = lambda_\n",
    "        self.max_iterations = max_iterations\n",
    "\n",
    "    def optimize(self):\n",
    "        optimizer = torch.optim.SGD([self.params], lr=0.01)  # Dummy optimizer, as we manually perform updates\n",
    "        \n",
    "        for iteration in range(self.max_iterations):\n",
    "            optimizer.zero_grad()\n",
    "            loss = self.target_func(self.model(self.params))\n",
    "            loss_list.append(loss)\n",
    "            print(loss)\n",
    "            loss.backward()\n",
    "\n",
    "            # Compute the approximate Hessian matrix\n",
    "            with torch.no_grad():\n",
    "                approx_hessian = torch.autograd.functional.hessian(self.target_func, self.params)\n",
    "                diag_values = approx_hessian.diagonal() + self.lambda_\n",
    "            # Update parameters using LM formula: theta_new = theta - (H + lambda*I)^(-1) * gradient\n",
    "            update = torch.linalg.solve(approx_hessian + torch.diag(diag_values), -self.params.grad)\n",
    "            self.params.data += update\n",
    "\n",
    "        return self.params.data.numpy()\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "# Define your target function and model appropriately\n",
    "def target_func(output):\n",
    "    # Define your loss function here, for example, mean squared error\n",
    "    loss = torch.mean((output - y_train.view(-1, 1))**2)\n",
    "    return loss\n",
    "\n",
    "# Instantiate your model and Levenberg-Marquardt optimizer\n",
    "model = MLP()\n",
    "initial_params = torch.empty(X_train.shape[1])\n",
    "initial_params = nn.init.uniform_(initial_params)\n",
    "#initial_params = torch.tensor([1.0] * 375)  # Initial guess for parameters\n",
    "lm_optimizer = LevenbergMarquardtOptimizer(model, target_func, model.parameters())\n",
    "\n",
    "# Perform optimization\n",
    "optimized_params = lm_optimizer.optimize()\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 986,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/300], Loss: 0.1895\n",
      "Epoch [20/300], Loss: 0.1269\n",
      "Epoch [30/300], Loss: 0.0862\n",
      "Epoch [40/300], Loss: 0.0668\n",
      "Epoch [50/300], Loss: 0.0564\n",
      "Epoch [60/300], Loss: 0.0498\n",
      "Epoch [70/300], Loss: 0.0453\n",
      "Epoch [80/300], Loss: 0.0419\n",
      "Epoch [90/300], Loss: 0.0392\n",
      "Epoch [100/300], Loss: 0.0370\n",
      "Epoch [110/300], Loss: 0.0352\n",
      "Epoch [120/300], Loss: 0.0336\n",
      "Epoch [130/300], Loss: 0.0322\n",
      "Epoch [140/300], Loss: 0.0310\n",
      "Epoch [150/300], Loss: 0.0300\n",
      "Epoch [160/300], Loss: 0.0291\n",
      "Epoch [170/300], Loss: 0.0282\n",
      "Epoch [180/300], Loss: 0.0275\n",
      "Epoch [190/300], Loss: 0.0268\n",
      "Epoch [200/300], Loss: 0.0262\n",
      "Epoch [210/300], Loss: 0.0256\n",
      "Epoch [220/300], Loss: 0.0251\n",
      "Epoch [230/300], Loss: 0.0246\n",
      "Epoch [240/300], Loss: 0.0242\n",
      "Epoch [250/300], Loss: 0.0238\n",
      "Epoch [260/300], Loss: 0.0235\n",
      "Epoch [270/300], Loss: 0.0231\n",
      "Epoch [280/300], Loss: 0.0228\n",
      "Epoch [290/300], Loss: 0.0225\n",
      "Epoch [300/300], Loss: 0.0222\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import Optimizer\n",
    "\n",
    "\n",
    "def target_func(output):\n",
    "    return criterion(outputs, y_train.view(-1, 1))\n",
    "\n",
    "class LevenbergMarquardt(Optimizer):\n",
    "    def __init__(self, params, lr=1, lambda_=0.01):\n",
    "        if lr < 0.0:\n",
    "            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n",
    "        if lambda_ < 0.0:\n",
    "            raise ValueError(\"Invalid lambda value: {}\".format(lambda_))\n",
    "        \n",
    "        defaults = dict(lr=lr, lambda_=lambda_)\n",
    "        super(LevenbergMarquardt, self).__init__(params, defaults)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            with torch.enable_grad():\n",
    "                loss = closure()\n",
    "                \n",
    "        for group in self.param_groups:\n",
    "            for param in group['params']:\n",
    "                if param.grad is None:\n",
    "                    continue\n",
    "\n",
    "                lr = group['lr']\n",
    "                lambda_ = group['lambda_']\n",
    "                if len(param.shape) == 2:\n",
    "                    d_p = param.grad\n",
    "                    d_p = torch.reshape(d_p, (-1,))\n",
    "                    shape0, shape1 = param.shape[0],param.shape[1]\n",
    "                    param = torch.reshape(param, (-1,))\n",
    "                    # Compute the approximate Hessian matrix (second derivative)\n",
    "                    approx_hessian = torch.autograd.functional.hessian(func=target_func,inputs=param,create_graph=True)\n",
    "                    diag_values = torch.diag(approx_hessian) + lambda_\n",
    "                    # Update parameters using LM formula: theta_new = theta - (H + lambda*I)^(-1) * gradient\n",
    "                    update = torch.linalg.solve(approx_hessian + torch.diag(diag_values), -d_p)\n",
    "                    param.data += lr * update\n",
    "                    param = torch.reshape(param,(shape0,shape1))\n",
    "                else:\n",
    "                    d_p = param.grad\n",
    "                    approx_hessian = torch.autograd.functional.hessian(func=target_func,inputs=param,create_graph=True)\n",
    "                    diag_values = torch.diag(approx_hessian) + lambda_\n",
    "                    # Update parameters using LM formula: theta_new = theta - (H + lambda*I)^(-1) * gradient\n",
    "                    update = torch.linalg.solve(approx_hessian + torch.diag(diag_values), -d_p)\n",
    "                    param.data += lr * update\n",
    "\n",
    "        return loss\n",
    "\n",
    "model = MLP()\n",
    "lm_optimizer = LevenbergMarquardt(model.parameters(), lr=0.001, lambda_=0.001)\n",
    "loss_list = []\n",
    "num_epochs = 300\n",
    "for epoch in range(num_epochs):\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, y_train.view(-1, 1))      \n",
    "    loss_list.append(loss.item())  \n",
    "    lm_optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    lm_optimizer.step()\n",
    "    if (epoch+1)%10 == 0:\n",
    "        print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 987,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test data: 0.9648\n",
      "Correct: 219 Incorrect 8\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    predictions = model(X_test)\n",
    "    predictions = (predictions > 0.5).float()\n",
    "    accuracy = torch.sum(predictions.view(-1) == y_test).item() / y_test.size(0)\n",
    "    print(f'Accuracy on test data: {accuracy:.4f}')\n",
    "    print('Correct:',torch.sum(predictions.view(-1) == y_test).item(),\"Incorrect\",y_test.size(0)-torch.sum(predictions.view(-1) == y_test).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 988,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.linspace(1, len(loss_list) ,len(loss_list))\n",
    "y = loss_list\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(x, y, label='loss', color='m', linewidth=1.5)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Loss Curve')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1c612fed8d120ac5c30643c808a4d3b681130bd8cc40d3014416815b15cd817e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
